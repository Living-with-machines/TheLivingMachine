{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3289e1",
   "metadata": {},
   "source": [
    "# Process datasets\n",
    "\n",
    "This notebook takes as input a file with sentences containing the target query and:\n",
    "   \n",
    "1. Performs linguistic-based filtering of the sentences,\n",
    "2. Uses BLERT to perform word prediction of the masked target query.\n",
    "    \n",
    "The output are two additional files: `{dataset}_{query}_synparsed.pkl` and `{dataset}_{query}_synparsed_pred_bert.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef35344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from utils import explore_preds, prepare_sents\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the datasets here:\n",
    "datasets = [\"example\"] # in our experiments, this was [\"jsa\", \"hmd\", \"blb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the query tokens here. Change the query to see the results for a different\n",
    "# target word:\n",
    "query = \"machine\" \n",
    "min_year = 1783\n",
    "max_year = 1908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary maps the query to the name that will be displayed in the output file.\n",
    "generic = {\"machine\": \"machine\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary maps the query to the tokens it will be expanded to.\n",
    "query_tokens = dict()\n",
    "query_tokens[\"machine\"] = [\"machine\", \"machines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ----------------------------------\n",
    "#### Linguistic filtering\n",
    "print(\"Linguistic filtering\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"*\", dataset)\n",
    "    syndf = pd.read_csv(\"data/\" + dataset + \"_processed/\" + dataset + \"_\" + query + \".tsv\", sep=\"\\t\")\n",
    "    # If we have more than 100000 sentences, downsample to 100000:\n",
    "    if query != \"machine\" and syndf.shape[0] > 65000:\n",
    "        syndf = syndf.sample(n=65000, random_state=42)\n",
    "    # Get a sentence ID\n",
    "    syndf['sentId'] = list(syndf.index.values)\n",
    "    # Process and filter sentences through syntactic parsing:\n",
    "    syndf['currentSentence'] = syndf.apply(lambda x: prepare_sents.remove_punctspaces(x[\"currentSentence\"]), axis=1)\n",
    "    syndf['synt'] = prepare_sents.preprocess_pipe(syndf['currentSentence'], nlp)\n",
    "    syndf = syndf[syndf.apply(lambda x: prepare_sents.filter_sents_synt(x.synt, x.maskedSentence, x.currentSentence, x.targetExpression), axis=1)]\n",
    "    syndf[\"query_label\"] = syndf.apply(lambda x: prepare_sents.find_query_deplabel(x.synt, x.maskedSentence, x.targetExpression), axis=1)\n",
    "    syndf.to_pickle(\"data/\" + dataset + \"_processed/\" + dataset + \"_\" + query + \"_synparsed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c42ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ----------------------------------\n",
    "#### BERT masking\n",
    "\n",
    "print(\"BERT masking\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    dataset_path = \"data/\" + dataset + \"_processed/\" + dataset + \"_\" + query + \"_synparsed.pkl\"\n",
    "    \n",
    "    if not Path(dataset_path.split(\".pkl\")[0] + \"_pred_bert.pkl\").is_file():\n",
    "\n",
    "        # Load dataframe where to apply this:\n",
    "        pred_df = pd.read_pickle(dataset_path)\n",
    "        for epoch in  [\"1760_1850\", \"1890_1900\"]:\n",
    "\n",
    "            print(\"*\", epoch)\n",
    "\n",
    "            # Create pipeline depending on the BERT model of the specified period\n",
    "            # and the number of expected predictions:\n",
    "            pred_toks = 20\n",
    "            model_rd = explore_preds.create_mask_pipeline(epoch, pred_toks)\n",
    "\n",
    "            # Use BERT to find most likely predictions for a mask:\n",
    "            pred_df[\"pred_bert_\" + epoch] = pred_df.apply(lambda x: explore_preds.bert_masking(x, model_rd), axis=1)\n",
    "\n",
    "        pred_df.to_pickle(dataset_path.split(\".pkl\")[0] + \"_pred_bert.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39tlm)",
   "language": "python",
   "name": "py39tlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
