{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "british-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tools import animacy_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Choose scenario:\n",
    "\n",
    "prediction_approach = \"bert_masking\"\n",
    "# Options:\n",
    "# * \"bert_masking\" (string): BERT masking approach\n",
    "\n",
    "context = \"both\"\n",
    "# Options:\n",
    "# * \"sent\" (string): use the sentence where the target expression is located as input.\n",
    "# * \"both\" (string): use the sentence where the target expression is located, plus\n",
    "#                    the previous and next sentences, as input.\n",
    "\n",
    "time_period = \"before1850\"\n",
    "# Options:\n",
    "# * \"contemporary\" (string): bert-base-uncased (contemporary BERT)\n",
    "# * \"before1850\" (string, only for 19thC Machines): BERT trained on pre-1850 data\n",
    "# * \"from1850to1875\" (string, only for 19thC Machines): BERT trained on data from 1850 to 1875\n",
    "# * \"from1875to1890\" (string, only for 19thC Machines): BERT trained on data from 1875 to 1890\n",
    "# * \"from1890to1900\" (string, only for 19thC Machines): BERT trained on data from 1890 to 1900\n",
    "# * \"timeSensitive\" (string, only for 19thC Machines): BERT model appropriate fine-tuned BERT model of the period to which each sentence belongsto\n",
    "\n",
    "weighted = True\n",
    "# Options:\n",
    "# * True (boolean): animacy values of predicted tokens are averaged weighted by their probability score.\n",
    "# * False (boolean): animacy values of predicted tokens are averaged not weighted by their probability score.\n",
    "\n",
    "wsd = \"False\"\n",
    "# Options:\n",
    "# * \"False\" (string): first sense in WordNet\n",
    "# * \"bert\" (string): use BERT-adapted Lesk algorithm to perform sense disambiguation from WordNet\n",
    "\n",
    "words_cutoff = 100\n",
    "# Words cutoff (integer): number of predictions for a given MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Instantiate default values:\n",
    "language_model = None\n",
    "\n",
    "bert_models = {\"contemporary\": BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "               , \"before1850\": BertForMaskedLM.from_pretrained(\"../models/FT_bert_base_uncased_before_1850\")\n",
    "               , \"from1850to1875\": BertForMaskedLM.from_pretrained(\"../models/FT_bert_base_uncased_after_1850_before_1875\")\n",
    "               , \"from1875to1890\": BertForMaskedLM.from_pretrained(\"../models/FT_bert_base_uncased_after_1875_before_1890\")\n",
    "               , \"from1890to1900\": BertForMaskedLM.from_pretrained(\"../models/FT_bert_base_uncased_after_1890_before_1900\")\n",
    "              }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # BERT tokenizer is always the same\n",
    "\n",
    "wsdmodel = None\n",
    "if wsd == \"bert\":\n",
    "    wsdmodel = SentenceTransformer('../models/language_models/bert_models/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aware-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Load dataset and run:\n",
    "dataset_df = pd.read_pickle(\"../data/jsa_animacy.pkl\")\n",
    "exp_path = \"../experiments/\" + context + \"_wordsCutoff\" + str(words_cutoff) + \"_\" + prediction_approach + \"_wsd\" + str(wsd) + \"_\" + time_period + \".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "median-clone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "animacy_detection.predict_mask_animacy(exp_path, dataset_df, context, words_cutoff, prediction_approach, wsd, wsdmodel, tokenizer, language_model, bert_models, time_period, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "returning-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.read_pickle(exp_path)\n",
    "predictions_df[\"animacy_score_20pr\"] = predictions_df.apply(lambda row: animacy_detection.animacy_score(row[\"predicted\"], row[\"scores\"], weighted, 20), axis=1)\n",
    "predictions_df[\"animacy_score_100pr\"] = predictions_df.apply(lambda row: animacy_detection.animacy_score(row[\"predicted\"], row[\"scores\"], weighted, 100), axis=1)\n",
    "predictions_df.to_csv(\"../experiments/predicted_\" + context + \"_wordsCutoff\" + str(words_cutoff) + \"_\" + prediction_approach + \"_wsd\" + str(wsd) + \"_\" + time_period + \".tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-shore",
   "metadata": {},
   "source": [
    "### WHAT DO WE DO WITH COMPOUND NOUNS, EG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"His patent expanding piston has been employed by them in steam engines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-approval",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37animacy)",
   "language": "python",
   "name": "py37animacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
