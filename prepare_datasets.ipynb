{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c22c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from  pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b37ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "cosine_similarity = lambda x, target : 1 - cosine(x,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd92891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_jsa\n",
    "from utils import process_rsc\n",
    "from utils import prepare_sents\n",
    "from utils import explore_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954daa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a word2vec model\n",
    "path2model = \"./models/word2vec/w2v_1760_1900/w2v_words.model\"\n",
    "w2v_model = Word2Vec.load(path2model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163ca76",
   "metadata": {},
   "source": [
    "#### Process  the JSA corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../../workspace/data/\" # Path where JSA data is located\n",
    "output_path = \"data/jsa_processed/\"\n",
    "overwrite = False # If False, run the code only if output has not been created.\n",
    "                  # If True, run the code regardless.\n",
    "\n",
    "process_jsa.parse_corpus(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2c3a9",
   "metadata": {},
   "source": [
    "#### Process the RSC corpus\n",
    "\n",
    "Data downloaded from https://fedora.clarin-d.uni-saarland.de/rsc_v6/access.html#download.\n",
    "\n",
    "We are using:\n",
    "* TEI-formatted corpus [v6.0.4](https://fedora.clarin-d.uni-saarland.de/rsc_v6/data/texts/Royal_Society_Corpus_open_v6.0.4_texts_tei.zip) (as separate files).\n",
    "* Corresponding metadata [v6.0.4](https://fedora.clarin-d.uni-saarland.de/rsc_v6/data/Royal_Society_Corpus_open_v6.0.4_meta.tsv.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../../workspace/data/RSC/\" # Path where JSA data is located\n",
    "output_path = \"data/rsc_processed/\"\n",
    "overwrite = False # If False, run the code only if output has not been created.\n",
    "                  # If True, run the code regardless.\n",
    "\n",
    "process_rsc.parse_corpus(input_path, output_path, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b35e70",
   "metadata": {},
   "source": [
    "#### Get sentences with machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7902d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the query tokens here:\n",
    "query_tokens = [\"machine\", \"machines\", \"engine\", \"engines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"JSA\"\n",
    "jsa_sents_df = prepare_sents.filter_sents_query(corpus, query_tokens)\n",
    "jsa_sents_df.to_csv(\"data/jsa_processed/JSA_machines.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ffb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"RSC\"\n",
    "rsc_sents_df = prepare_sents.filter_sents_query(corpus, query_tokens)\n",
    "rsc_sents_df = rsc_sents_df[(rsc_sents_df[\"year\"] >= 1783) & (rsc_sents_df[\"year\"] <= 1908)]\n",
    "rsc_sents_df.to_csv(\"data/rsc_processed/RSC_machines.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe841b",
   "metadata": {},
   "source": [
    "#### Syntactic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "syndf = jsa_sents_df.copy()\n",
    "syndf['synt'] = prepare_sents.preprocess_pipe(syndf['currentSentence'], nlp)\n",
    "syndf = syndf[syndf.apply(lambda x: prepare_sents.filter_sents_synt(x.synt, x.targetExpression), axis=1)]\n",
    "syndf[\"query_label\"] = syndf.apply(lambda x: prepare_sents.find_query_deplabel(x.synt, x.maskedSentence, x.targetExpression), axis=1)\n",
    "syndf.to_pickle(\"data/jsa_processed/JSA_synparsed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b740743",
   "metadata": {},
   "outputs": [],
   "source": [
    "syndf = rsc_sents_df.copy()\n",
    "syndf['synt'] = prepare_sents.preprocess_pipe(syndf['currentSentence'], nlp)\n",
    "syndf = syndf[syndf.apply(lambda x: prepare_sents.filter_sents_synt(x.synt, x.targetExpression), axis=1)]\n",
    "syndf[\"query_label\"] = syndf.apply(lambda x: prepare_sents.find_query_deplabel(x.synt, x.maskedSentence, x.targetExpression), axis=1)\n",
    "syndf.to_pickle(\"data/rsc_processed/RSC_synparsed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd674fad",
   "metadata": {},
   "source": [
    "#### BERT masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"data/rsc_processed/RSC_synparsed.pkl\",\n",
    "                \"data/jsa_processed/JSA_synparsed.pkl\"]:\n",
    "    \n",
    "    if not Path(dataset.split(\".pkl\")[0] + \"_pred_bert.pkl\").is_file():\n",
    "        \n",
    "        print(dataset)\n",
    "\n",
    "        # Load dataframe where to apply this:\n",
    "        pred_df = pd.read_pickle(dataset)\n",
    "        for epoch in [\"1760_1850\", \"1850_1875\", \"1875_1890\", \"1890_1900\"]:\n",
    "\n",
    "            print(epoch)\n",
    "\n",
    "            # Create pipeline depending on the BERT model of the specified period\n",
    "            # and the number of expected predictions:\n",
    "            pred_toks = 10\n",
    "            model_rd = explore_preds.create_mask_pipeline(epoch, pred_toks)\n",
    "\n",
    "            # Use BERT to find most likely predictions for a mask:\n",
    "            pred_df[\"pred_bert_\" + epoch] = pred_df.apply(lambda x: explore_preds.bert_masking(x, model_rd), axis=1)\n",
    "\n",
    "        pred_df.to_pickle(dataset.split(\".pkl\")[0] + \"_pred_bert.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38_histLM)",
   "language": "python",
   "name": "py38_histlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
